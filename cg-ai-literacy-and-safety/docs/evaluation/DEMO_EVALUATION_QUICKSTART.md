# Demo Evaluation Quick Start Guide

## 5-Minute Assessment

Use this for a rapid initial evaluation before diving deep.

### 1. Foundation Check (1 min)
```
â–¡ UNESCO: Does it teach human agency, ethics, techniques, design?
â–¡ OECD: Does it cover all 5 principles (growth, human values, transparency, safety, accountability)?
â–¡ NIST: Can players map context, measure risk, manage decisions, govern trust?
â–¡ EU: Are all 7 Trustworthy AI requirements addressed?
```

### 2. Game Loop Check (1 min)
```
â–¡ Is the core loop: Input â†’ Decision â†’ Feedback â†’ Progress?
â–¡ Are decisions meaningful (not obvious)?
â–¡ Is feedback immediate and educational?
â–¡ Can players recover from mistakes?
```

### 3. Scenario Check (1 min)
```
â–¡ Are there at least 10 scenarios?
â–¡ Do scenarios cover different domains (health, finance, justice, etc.)?
â–¡ Is there a mix of AI-correct and AI-wrong cases?
â–¡ Are consequences shown to players?
```

### 4. Engagement Check (1 min)
```
â–¡ Is there a progression system?
â–¡ Are concepts being tracked/unlocked?
â–¡ Is there a story or narrative framing?
â–¡ Would players want to replay?
```

### 5. Technical Check (1 min)
```
â–¡ Does it work offline?
â–¡ Is it mobile-friendly?
â–¡ Are there keyboard shortcuts?
â–¡ Does it load quickly (< 3 seconds)?
```

---

## Priority Classification Guide

### ðŸ”´ Critical (Fix First)
- Missing framework alignment (UNESCO/OECD/NIST/EU gaps)
- No feedback mechanism
- Broken core game loop
- No learning objectives met
- Technical blockers

**Examples:**
- No explanation of why AI was wrong
- Missing privacy scenario (EU gap)
- Scenarios don't teach any concepts
- Game crashes or won't load

### ðŸŸ¡ Important (Fix Soon)
- Limited replayability
- No difficulty options
- Missing assessment/quizzes
- No multiplayer/collaboration
- Poor mobile experience

**Examples:**
- Same 10 scenarios every time (no randomization)
- One difficulty level only
- No way to verify learning
- Can't play with friends

### ðŸŸ¢ Enhancement (Nice to Have)
- Narrative framing
- Sandbox/custom modes
- Accessibility features
- Leaderboards
- Achievement system

**Examples:**
- Add mentor character
- Let players create scenarios
- Add screen reader support
- Global high scores

---

## Common Improvements by Demo Type

### Decision-Making Games (like Human-in-the-Loop)
```
Critical:
â–¡ Consequence visualization (show what happened)
â–¡ Multiple difficulty levels
â–¡ Concept mastery verification (quizzes)

Important:
â–¡ Collaborative/multiplayer mode
â–¡ Time pressure (Expert mode)
â–¡ Scenario randomization

Enhancement:
â–¡ Narrative framing (certification story)
â–¡ Sandbox mode (create scenarios)
â–¡ Real-world connection module
```

### Detection Games (like Deepfake Detective)
```
Critical:
â–¡ Progressive difficulty
â–¡ Clear feedback on misses
â–¡ Multiple example types

Important:
â–¡ Hint system
â–¡ Practice mode
â–¡ Performance tracking

Enhancement:
â–¡ Competitive mode
â–¡ Community examples
â–¡ Certification path
```

### Investigation Games (like Bias Bounty)
```
Critical:
â–¡ Clear investigation mechanics
â–¡ Multiple bias patterns
â–¡ Evidence linking

Important:
â–¡ Team collaboration
â–¡ Case variety
â–¡ Hint system

Enhancement:
â–¡ Custom cases
â–¡ Leaderboards
â–¡ Narrative wrapper
```

---

## File Structure Template

```
demos/[demo-name]/
â”œâ”€â”€ EVALUATION_REPORT.md          # Comprehensive evaluation
â”œâ”€â”€ IMPROVEMENT_TRACKING.md       # Progress tracking
â”œâ”€â”€ IMPLEMENTATION_SUMMARY_v1.0.md # Baseline
â”œâ”€â”€ IMPLEMENTATION_SUMMARY_v1.1.md # Critical items
â”œâ”€â”€ IMPLEMENTATION_SUMMARY_v1.2.md # Important items
â”œâ”€â”€ IMPLEMENTATION_SUMMARY_v1.3.md # Enhancements
â””â”€â”€ web-vanilla/                  # Implementation
    â”œâ”€â”€ index.html
    â”œâ”€â”€ styles.css
    â”œâ”€â”€ app.js
    â”œâ”€â”€ scenarios.json
    â”œâ”€â”€ quiz_data.js              # If applicable
    â”œâ”€â”€ narrative_data.js         # If applicable
    â””â”€â”€ collaborative_mode.js     # If applicable
```

---

## Quick Metrics Dashboard

Track these numbers:

| Metric | Target | Current |
|--------|--------|---------|
| Scenarios | 15+ | |
| Concepts | 8+ | |
| Replay Value | High | |
| Framework Coverage | 90%+ | |
| Overall Score | 8.5+/10 | |

---

## Decision Tree

```
START: Evaluate Demo
â”‚
â”œâ”€ Is framework alignment < 80%?
â”‚  â””â”€ YES â†’ Priority: Critical
â”‚
â”œâ”€ Are there < 10 scenarios?
â”‚  â””â”€ YES â†’ Priority: Critical
â”‚
â”œâ”€ Is there no feedback mechanism?
â”‚  â””â”€ YES â†’ Priority: Critical
â”‚
â”œâ”€ Is replayability low?
â”‚  â””â”€ YES â†’ Priority: Important
â”‚
â”œâ”€ Is there no assessment?
â”‚  â””â”€ YES â†’ Priority: Important
â”‚
â””â”€ Is narrative missing?
   â””â”€ YES â†’ Priority: Enhancement
```

---

## One-Page Cheat Sheet

### Top 5 Improvements (Most Impact)
1. **Add consequence visualization** - Show players the real-world impact
2. **Implement adaptive difficulty** - Easy/Normal/Expert modes
3. **Add concept quizzes** - Verify learning, not just exposure
4. **Enable scenario randomization** - Increase replayability
5. **Add narrative framing** - Certification/story arc

### Top 5 Common Gaps
1. Missing privacy scenarios (EU framework)
2. No environmental/sustainability content
3. Static scenarios (same order every time)
4. No multiplayer/collaboration
5. Limited accessibility features

### Top 5 Quick Wins
1. Add keyboard shortcuts
2. Show progress counter
3. Add concept icons
4. Improve color contrast
5. Add loading animation

---

## Example: Human-in-the-Loop Results

| Phase | Items | Time | Impact |
|-------|-------|------|--------|
| v1.1 Critical | 3 items | 6 hours | +0.6 score |
| v1.2 Important | 3 items | 6 hours | +0.3 score |
| v1.3 Enhancement | 1 item | 2 hours | +0.1 score |
| **Total** | **7 items** | **14 hours** | **+1.0 score (8.0 â†’ 9.0)** |

---

## Next Steps Checklist

- [ ] Read full `DEMO_EVALUATION_AND_IMPROVEMENT_PROMPT.md`
- [ ] Evaluate demo using Phase 1-4
- [ ] Create EVALUATION_REPORT.md
- [ ] Create IMPROVEMENT_TRACKING.md
- [ ] Identify Critical items (top 3)
- [ ] Set version targets (v1.1, v1.2, etc.)
- [ ] Begin implementation
- [ ] Track progress in IMPROVEMENT_TRACKING.md
- [ ] Create IMPLEMENTATION_SUMMARY after each release

---

*Start with the 5-minute assessment, then dive deep with the full prompt.*
